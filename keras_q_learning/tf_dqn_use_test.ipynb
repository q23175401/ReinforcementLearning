{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow module\n",
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# import tensorflow reinforcement learning modules\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.agents.ppo import ppo_agent, ppo_actor_network\n",
    "from tf_agents.networks import value_network, actor_distribution_network\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import sequential\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.replay_buffers.tf_uniform_replay_buffer import TFUniformReplayBuffer\n",
    "from tf_agents.policies import PolicySaver\n",
    "# import gym\n",
    "\n",
    "# import useful module\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image as plImage\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "測試import有無成功"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test if the module being imported successfully # 可以註解掉\n",
    "\n",
    "# env_name = 'CartPole-v0'\n",
    "# env = suite_gym.load(env_name)\n",
    "# # env.reset()                     # 回到初始state\n",
    "# # plImage.fromarray(env.render()) # 可以秀出當前state的圖片\n",
    "# # env.close()                     # 要關閉才不會當掉\n",
    "# #由環境本身紀錄的各種規格 => 用於創建神經網路(知道輸入跟輸出的各種資訊)\n",
    "\n",
    "# print('All Spec at this time step: \\n', env.time_step_spec(), '\\n')   # 這個時間點得到的各種東西的規格)\n",
    "# print('Observation Spec: \\n', env.time_step_spec().observation, '\\n') # 此 Env 輸出的 state 會是這個規格\n",
    "# print('Reward Spec: \\n', env.time_step_spec().reward, '\\n')           # 此 Env 上一個action所造成的 reward 的規格\n",
    "# print('Action Spec: \\n', env.action_spec(), '\\n')                     # 要輸入此 Env 的 action 要符合的規格\n",
    "\n",
    "# # 由環境真實輸出的state (會符合上方的規格))\n",
    "# time_step = env.reset()\n",
    "# print('Time step:\\n', time_step, '\\n')\n",
    "\n",
    "# # take an action \n",
    "# action = np.array(1, dtype=np.int32)\n",
    "# next_time_step = env.step(action)\n",
    "# print('Next time step:\\n', next_time_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from connextx_game_board import GameBoardEnv\n",
    "from connectx_agents import agent_random, MinimaxAgent\n",
    "from tf_agents.environments import TFPyEnvironment\n",
    "from tf_agents.environments.gym_wrapper import GymWrapper\n",
    "\n",
    "def CreateTFGameBoardEnv(opponent_agent):\n",
    "        gym_env        = GameBoardEnv(opponent_agent)   # 一開始是Gym架構的Env\n",
    "        suit_gym_env   = GymWrapper(gym_env)            # 轉成tensorflow包裝好的Suit gym env架構\n",
    "        tf_tensor_env  = TFPyEnvironment(suit_gym_env)  # 再轉成tf tensor env\n",
    "        return tf_tensor_env\n",
    "\n",
    "def create_my_train_evaluate_envs(opponent_agent):\n",
    "    train_env = CreateTFGameBoardEnv(opponent_agent)\n",
    "    evaluate_env = CreateTFGameBoardEnv(opponent_agent)\n",
    "    return train_env, evaluate_env\n",
    "\n",
    "# train_env, evaluate_env = create_my_train_evaluate_envs(agent_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 由於要用 DQN演算法 要建立 Qnet 以及建立 DQN Agent的部分\n",
    "# action_tensor_spec = tensor_spec.from_spec(env.action_spec())               # 從測試用的env取得spec 再轉換成tensorflow架構\n",
    "# action_tensor_spec = train_env.action_spec()                                # 也可以直接從剛剛轉換成tf_Env的環境取得tensorflow架構的spec\n",
    "# num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1   # 根據spec算出總共有多少可使用的action\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten, BatchNormalization, Activation\n",
    "from tensorflow.keras.activations import softmax, tanh\n",
    "\n",
    "def dense_layer(num_output_units): # 設定輸出的數值數量\n",
    "    return Dense(\n",
    "        num_output_units,\n",
    "        activation=tf.keras.activations.relu,\n",
    "        kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0, mode='fan_in', distribution='truncated_normal')\n",
    "    )\n",
    "# QNetwork consists of a sequence of Dense layers followed by a dense layer\n",
    "# with `num_actions` units to generate one q_value per available action as it's output.\n",
    "def create_my_qnetwork(num_output):\n",
    "    # Define a helper function to create Dense layers configured with the right activation and kernel initializer.\n",
    "    \n",
    "    layer_list = [\n",
    "        # Conv2D(16, (3, 3), padding='same', strides=(2, 2), activation='relu'),\n",
    "        # Conv2D(8, (3, 3), padding='same', strides=(2, 2), activation='relu'),\n",
    "        Flatten(),\n",
    "        dense_layer(128),\n",
    "        dense_layer(64),\n",
    "        Dense(\n",
    "            num_output,                     # 最後有幾個action就輸出幾個數值\n",
    "            activation=None,                # q net 最後輸出 意義是每個動作預期的reward\n",
    "            kernel_initializer=tf.keras.initializers.RandomUniform(minval=-0.03, maxval=0.03),\n",
    "            bias_initializer=tf.keras.initializers.Constant(-0.2),\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    q_net = sequential.Sequential(layer_list)\n",
    "    return q_net\n",
    "\n",
    "def create_initialized_dqn_agent(time_step_spec, action_spec, q_net, learning_rate):\n",
    "    agent = dqn_agent.DqnAgent(\n",
    "        time_step_spec      = time_step_spec,\n",
    "        action_spec         = action_spec,\n",
    "        q_network           = q_net,\n",
    "        optimizer           = tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        td_errors_loss_fn   = common.element_wise_squared_loss,\n",
    "        train_step_counter  = tf.Variable(0)  # 看總共訓練幾次的計數器\n",
    "    )\n",
    "    #將所有的參數初始化\n",
    "    agent.initialize()\n",
    "\n",
    "    # initialize before training\n",
    "    agent.train = common.function(agent.train)     # 將訓練的function 加到tensor board graph中(可有可無)\n",
    "    agent.train_step_counter.assign(0)             # Reset the train step，初始化紀錄訓練次數的counter\n",
    "    return agent\n",
    "\n",
    "\n",
    "def create_my_actor_critic_net(train_env: TFPyEnvironment):\n",
    "    # actor_layer_list = [\n",
    "    #     Flatten(),\n",
    "    #     dense_layer(200),\n",
    "    #     dense_layer(100),\n",
    "    #     dense_layer(50),\n",
    "    #     dense_layer(num_output),\n",
    "    #     Activation(softmax)\n",
    "    # ]    \n",
    "    # actor_net = sequential.Sequential(actor_layer_list)\n",
    "\n",
    "    # critic_layer_list = [\n",
    "    #     Flatten(),\n",
    "    #     dense_layer(100),\n",
    "    #     dense_layer(50),\n",
    "    #     dense_layer(10),\n",
    "    #     dense_layer(1),\n",
    "    #     Activation(tanh)\n",
    "    # ]\n",
    "    # critic_net = sequential.Sequential(critic_layer_list)\n",
    "\n",
    "    # value_network, ppo_actor_network\n",
    "    actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "        train_env.time_step_spec().observation,\n",
    "        train_env.action_spec()\n",
    "    )\n",
    "\n",
    "    critic_net = value_network.ValueNetwork(\n",
    "        train_env.time_step_spec().observation\n",
    "    )\n",
    "    return actor_net, critic_net\n",
    "\n",
    "def create_initialized_ppo_agent(time_step_spec, action_spec, actor_net, critic_net, learning_rate):\n",
    "    agent = ppo_agent.PPOAgent(\n",
    "        time_step_spec     = time_step_spec,\n",
    "        action_spec        = action_spec,\n",
    "        value_net          = critic_net,\n",
    "        actor_net          = actor_net,\n",
    "        log_prob_clipping  = 1,\n",
    "        use_gae            = True,\n",
    "        train_step_counter = tf.Variable(0)\n",
    "    )\n",
    "\n",
    "    #將所有的參數初始化\n",
    "    agent.initialize()\n",
    "\n",
    "    # initialize before training\n",
    "    agent.train = common.function(agent.train)     # 將訓練的function 加到tensor board graph中(可有可無)\n",
    "    agent.train_step_counter.assign(0)             # Reset the train step，初始化紀錄訓練次數的counter\n",
    "\n",
    "    return agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 蒐集資料的部分\n",
    "def take_one_step(environment, policy):                     # 把當前的Env 根據某個policy採取一個動作\n",
    "    time_step = environment.current_time_step()             # get current state\n",
    "    action_step = policy.action(time_step)                  # get action of this state from the policy\n",
    "    next_time_step = environment.step(action_step.action)   # take action，環境的狀態真的會改變(並不只是看看下個狀態)\n",
    "    \n",
    "    return time_step, action_step, next_time_step           # 回傳current_state, action, next_state\n",
    "\n",
    "def collect_data(train_env, collect_policy, replay_buffer, num_steps):\n",
    "    for _ in range(num_steps):\n",
    "        state, action, next_state = take_one_step(train_env, collect_policy)\n",
    "\n",
    "        traj = trajectory.from_transition(state, action, next_state) # 把state action next_state存成一筆trajectory data\n",
    "        replay_buffer.add_batch(traj)                                # Add trajectory to the replay buffer\n",
    "\n",
    "def create_replay_buffer_and_training_iterater(traj_spec, train_env, buffer_length, initial_random_steps):\n",
    "    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer( # 要設定每一筆資料的格式\n",
    "        data_spec   = traj_spec,             # 每個演算法要的格式會不一樣，所以要跟每個演算法拿藥蒐集的資料格式，collect_data_spec是一個named tuple，名字是Trajectory\n",
    "        batch_size  = train_env.batch_size, # 一次要拿多少筆資料進去訓練\n",
    "        max_length  = buffer_length\n",
    "    )        # 最多有多少筆資料在Buffer裡面\n",
    "\n",
    "    #可以自己創造一個隨機選取動作的policy(由訓練環境的輸入和輸出規格設定)\n",
    "    random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(), train_env.action_spec()) \n",
    "    # random_policy.action(train_env.current_time_step()) # TF Policy吃的就是 TF time step決定他的action\n",
    "\n",
    "    # 在訓練前先蒐集一定數量的隨機動作\n",
    "    collect_data(train_env, random_policy, replay_buffer, initial_random_steps)\n",
    "\n",
    "    # 這個Buffer 要轉成tensorflow dataset的形式作為訓練model的data pipeline\n",
    "    training_dataset = replay_buffer.as_dataset(\n",
    "        num_parallel_calls  =3, \n",
    "        sample_batch_size   =train_env.batch_size, \n",
    "        num_steps           =2\n",
    "    ).prefetch(3)                # num step=2 是因為DQN一次需要這個state跟下個state計算loss\n",
    "\n",
    "    training_iterater = iter(training_dataset)  # 訓練的pipeline\n",
    "    return replay_buffer, training_iterater\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義要evaluate這個policy好不好的function，玩完一定的次數後算平均的reward\n",
    "\n",
    "def compute_average_return(environment, policy, num_episodes=10):\n",
    "    total_return = 0.0  \n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "        total_return += episode_return\n",
    "\n",
    "\n",
    "    average_return = total_return / num_episodes\n",
    "    return average_return.numpy()[0]\n",
    "# average_reward = compute_average_return(evaluate_env, random_policy, num_evaluate_episodes) # 測試是否成功\n",
    "\n",
    "def train_one_iteration(train_env, agent, replay_buffer, train_date_iterater, num_collect_steps):\n",
    "    collect_data(train_env, agent.collect_policy, replay_buffer, num_collect_steps) # 每一個訓練迴圈會利用collect data policy取得一些訓練資料\n",
    "    \n",
    "    # Sample a batch of data from the buffer and update the agent's network.\n",
    "    experience, __unused_info = next(train_date_iterater) # 根據訓練資料pipeline取得一筆訓練資料\n",
    "    training_loss = agent.train(experience).loss        # 用訓練資料做訓練並且算出訓練的loss\n",
    "    return training_loss\n",
    "\n",
    "def save_policy(policy_name, policy):\n",
    "    if not os.path.isdir(policy_name):\n",
    "        policy_saver = PolicySaver(policy, batch_size=None)\n",
    "        policy_saver.save(policy_name)\n",
    "    else:\n",
    "        print('Policy Exist Already')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting hyperparameters\n",
    "num_training_iterations = 50000      # @param {type:\"integer\"}   # 總共要訓練幾個迴圈\n",
    "batch_size = 64                      # @param {type:\"integer\"}   # \n",
    "learning_rate = 1e-3                 # @param {type:\"number\"}    # learning rate for optimizer of the net\n",
    "log_interval = 200                   # @param {type:\"integer\"}   # show loss after every x training steps \n",
    "\n",
    "# collect data parameters\n",
    "initial_collect_steps = 100          # @param {type:\"integer\"}   # warm up random steps to test the enviromments\n",
    "collect_steps_per_iteration = 1      # @param {type:\"integer\"}   # collect x step data every trainging loop \n",
    "replay_buffer_max_length = 10000     # @param {type:\"integer\"}   # \n",
    "\n",
    "# evaluate parameters\n",
    "num_evaluate_episodes = 10           # @param {type:\"integer\"}   # test current average reward of the current policy by playing x times\n",
    "evaluate_interval = 1000             # @param {type:\"integer\"}   # show average reward every x training steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train initialize process\n",
    "train_env, evaluate_env = create_my_train_evaluate_envs(opponent_agent=MinimaxAgent(1))\n",
    "\n",
    "action_tensor_spec = train_env.action_spec()                                # 也可以直接從剛剛轉換成tf_Env的環境取得tensorflow架構的spec\n",
    "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1   # 根據spec算出總共有多少可使用的action\n",
    "\n",
    "# # dqn method\n",
    "q_net = create_my_qnetwork(num_actions)\n",
    "\n",
    "agent = create_initialized_dqn_agent( \\\n",
    "      train_env.time_step_spec(),\n",
    "      train_env.action_spec(),\n",
    "      q_net,\n",
    "      learning_rate\n",
    "   )\n",
    "# actor_net, critic_net = create_my_actor_critic_net(train_env)\n",
    "# agent = \\\n",
    "#    create_initialized_ppo_agent(\n",
    "#       train_env.time_step_spec(),\n",
    "#       train_env.action_spec(),\n",
    "#       actor_net,\n",
    "#       critic_net,\n",
    "#       learning_rate\n",
    "#    )\n",
    "\n",
    "\n",
    "replay_buffer, training_iterater = \\\n",
    "   create_replay_buffer_and_training_iterater( \\\n",
    "      agent.collect_data_spec,\n",
    "      train_env,\n",
    "      replay_buffer_max_length,\n",
    "      initial_collect_steps\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average_return = compute_average_return(evaluate_env, agent.policy, num_evaluate_episodes)\n",
    "# returns = [average_return] # 儲存訓練過程中的平均分數成長\n",
    "# 定義一個整個訓練過程 (train_env, agent, replay_buffer, training_iterater, collect_steps_per_iteration, log_interval, evaluate_interval 要先定義好)\n",
    "def train_agent(iterations):\n",
    "    average_return = compute_average_return(evaluate_env, agent.policy, num_evaluate_episodes)\n",
    "    returns = [average_return] # 儲存訓練過程中的平均分數成長\n",
    "\n",
    "    for _ in range(iterations):\n",
    "\n",
    "        training_loss = train_one_iteration(train_env, agent, replay_buffer, training_iterater, collect_steps_per_iteration)\n",
    "\n",
    "        #看看訓練到哪步了\n",
    "        step = agent.train_step_counter.numpy()\n",
    "        if step % log_interval == 0:\n",
    "            print('step = {0}: loss = {1}'.format(step, training_loss))\n",
    "\n",
    "        if step % evaluate_interval == 0:\n",
    "            average_return = compute_average_return(evaluate_env, agent.policy, num_evaluate_episodes)\n",
    "            print('step = {0}: Average Return = {1}'.format(step, average_return))\n",
    "            returns.append(average_return)\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_env, evaluate_env = create_my_train_evaluate_envs(opponent_agent=MinimaxAgent(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練Agent\n",
    "\n",
    "#%% time # 用來輸出總訓練時間\n",
    "train_iterations = num_training_iterations*2\n",
    "\n",
    "returns = train_agent(train_iterations)\n",
    "\n",
    "#訓練完畢後，將訓練過程中計算的rewards 畫成折線圖顯示出來\n",
    "xrange_value = range(0, train_iterations+1, evaluate_interval)\n",
    "plt.ylabel('Average Return'); plt.xlabel('Iterations'); plt.plot(xrange_value, returns)\n",
    "\n",
    "# 訓練完畢就可以把訓練好的policy儲存起來\n",
    "save_policy(policy_name=f'DqnPolicy_{agent.train_step_counter.numpy()}', policy=agent.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 純粹試跑這個訓練好的Policy，與訓練無關，所以重新import\n",
    "\n",
    "import os\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "import PIL.Image as plImage\n",
    "import tensorflow as tf\n",
    "# \n",
    "try:\n",
    "    policy_name = 'DqnPolicy_%d' % 20000\n",
    "    env_name = 'CartPole-v0'\n",
    "    env = suite_gym.load(env_name)\n",
    "    evaluate_env = tf_py_environment.TFPyEnvironment(suite_gym.load(env_name))\n",
    "    saved_policy = tf.compat.v2.saved_model.load(policy_name)\n",
    "    # print(compute_average_return(evaluate_env, saved_policy, 10))\n",
    "\n",
    "    for ep in range(10):\n",
    "        env.reset()\n",
    "        time_step = evaluate_env.reset()\n",
    "        \n",
    "        while not time_step.is_last():\n",
    "\n",
    "            # env.render()\n",
    "            # plImage.fromarray(evaluate_env.render().numpy())\n",
    "            plImage.fromarray(env.render())\n",
    "            \n",
    "            action_step = saved_policy.action(time_step)\n",
    "            time_step = evaluate_env.step(action_step)\n",
    "            env.step(action_step.action.numpy()[0])\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "finally:\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gym\n",
    "\n",
    "# env = gym.make('CartPole-v0')\n",
    "# total_episode = 100\n",
    "\n",
    "# for episode in range(total_episode):\n",
    "#     env.reset()\n",
    "    \n",
    "#     for timestep in range(1, 1000+1):\n",
    "#         env.render()\n",
    "#         observation, reward, done, info = env.step(env.action_space.sample())\n",
    "#         if done:\n",
    "#                 print(\"Episode finished after {} timesteps\".format(timestep))\n",
    "#                 break\n",
    "#     break\n",
    "# env.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4ca19a1d1d1323e3301fdd9fcb97ce5a2228b6cc8684ba646f32c2d37c6666bf"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
